<font size = "5 px" face = "Heiti SC">

<style>
table {
margin: auto;
}
</style>

<body>
<div style = "line-height: 38px">
</body>

# **信源与信源编码** 
前面我们从随机变量的角度介绍了信息的熵与信息量。接下来我们要从信息传递的==源头==，即信源，开始研究一条信息是如何产生以及传递到接收的。
## 信源
- **离散无记忆信源（DMS）**：该信源每隔 $\small t$ 秒输出一个符号，且对之前输出的符号不加以存储。即，若信源$\small X$可能发出$\small n$个不同符号
   $$\small \Omega:=\{x_1,x_2,\cdots,x_n\},$$发出各个符号的概率分别为
  $$\small p(x_1),p(x_2),\cdots,p(x_n),$$各符号间**彼此不相关**，且满足：
  $$\small \begin{split}&p(x_i)\geqslant 0,\ i = 1, 2, \cdots, n,\\ &\sum_{i=1}^np(x_i) = 1.\end{split}$$信源发出的符号从有限集合$\small \Omega$中选择。

- **信源熵**： 离散无记忆信源的熵为：
$$\small H(X) = -\sum_{i=1}^n p(x_i)\log_2 p(x_i)\ (\color{blue}{\mathrm{bits}}).$$

- **信源速率**：$$\small v = \frac{H(X)}{t}\ (\color{blue}{\mathrm{bits/s}}).$$
  
> **注：** 离散无记忆信源是最简单的信源，并且该信源的数学模型就是一个**离散型随机变量**。

## **信源编码**
接下来的问题是，如何在计算机中表示信息？我们知道在计算机中，所有的量都被表示为二进制，因此为了能够传输信息我们首先需要将信源发出的**自然语言**转换成便于计算机存储与传输的**二进制比特位**，此过程称为**编码**。

- **==定义1.==** 在信息论中，用来表示一个输出符号的==一串比特位==称为一个**码字**。一个**编码**是由若干个码字组成的**向量**。

- **定长编码:** 假设我们需要对26个字母用比特位来表示，则因为
$$\small 2^5 = 32 > 26,$$
故每个字母可以用5位来**唯一地表示**。这便是定长编码（FLC），即每一个信源输出符号都使用**固定长度的比特位**来表示。
    - **码字长度**：对于定长编码，**唯一地**表示一个输出所需的==比特位数量==称为*码字长度*，用$\small R$表示：$$\small R = \log_2 n,$$若输出集元素总量$\small n$不是2的幂，则码长取$$\small R = \lfloor\log_2 n\rfloor + 1.$$

> - **==希望==**：我们希望用**尽可能少**的码长来表示信息，则面对大量数据信息如果编码长度小，则可以提高存储效率以及传输效率，这便是**==数据压缩==**。例如，对于26个字母，并不是每一个字母使用频率都一样，一般而言字母 x, q, z使用的较少，而 s, t, e 使用较为频繁。但如果使用定长编码，则每一个字母都要用5bits来表示，造成**浪费**！
> - **==想法==**：如果某个字母**使用越频繁**，那么就使用**越少**的码字来表示，而对于**不常用**的字母使用**较多**的码字来表示，以降低一条信息的整体码长。

- **可变长编码 (VLC)**： 当信源符号**不是等可能**的情况时，可使用更为**高效**的可变长度编码，即每个符号不一定使用相同的码长来表示。

- **示例 1：** 字母A-H的定长编码：

    | Letter | Codeword | Letter | Codeword |
    | :-: | :-: | :-: | :-: |
    | A | 000 | E | 100 |
    | B | 001 | F | 101 |
    | C | 010 | G | 110 |
    | D | 011 | H | 111 |
    
- **示例 2:** 第一种可变长度编码方式：

    | Letter | Codeword | Letter | Codeword |
    | :-: | :-: | :-: | :-: |
    | A | 00  | E | 101  |
    | B | 010 | F | 110  |
    | C | 011 | G | 1110 |
    | D | 100 | H | 1111 |
 假设需要编码的字母序列为："A BAD CAB"，则

|编码方式|编码|总码长
| :-: | :-: | :-: |
| 定长编码 | 000 001 000 011 010 000 001 | 21 bits |
| 可变长度编码 | 00 010 00 100 011 00 010 | 18 bits |

- **示例 3:** 第二种可变长度编码方式：

    | Letter | Codeword | Letter | Codeword |
    | :-: | :-: | :-: | :-: |
    | A | 0   | E | 10   |
    | B | 1   | F | 11   |
    | C | 00  | G | 000  |
    | D | 01  | H | 111  |
    则编码同样的字母串会使用更少的码长：
    
|编码方式|编码|总码长
| :-: | :-: | :-: |
| 可变长度编码1 | 00 010 00 100 011 00 010 | 18 bits |
| 可变长度编码2 | 0 1001 0001              | 9  bits |

> - **==出现BUG！==** 考虑编码方式 VLC2，对于编码断句方式不同，可以得到不同的符号！同样是可变编码，VLC1 则没有这种情况，它只有一种有效的断句方式。

- **前缀条件**： 在编码中，没有码字能够形成其他码字的前缀。
- **前缀码**： 满足前缀条件的编码称为==**前缀码**==。对前缀码译码时不需要获得所有编码传输，即译码时==**没有延迟**==。故前缀码又称==**瞬时编码**==。
- **前缀码的==充要条件==**：对于$\small L$个码字，长度满足$$\small l_1\leqslant l_2\leqslant \cdots \leqslant l_L,$$则编码为前缀码的充要条件为：$$\small \sum_{i=1}^L 2^{-l_i} \leqslant 1.$$该不等式也称为 Kraft 不等式。显然，==任意前缀码都是唯一可译码的！==

> - 显然，VLC1 满足前缀条件，而 VLC2 *不*满足前缀条件！实际中尽管 VCL2 使用的比特数少但依然是无效的，因为它==不可唯一译码！==
> - 由于可变长度编码对应每一个不同的信源符号可能有不同的码长，接下来我们讨论如何得到可变的、唯一可译码的==平均==码字长度。

- **平均码字长度**：假设离散无记忆信源 $\small X$ 从一个有限符号集合中输出符号 $\small x_i,\ i = 1,2,\cdots, L$, 对应每个符号输出的概率为 $\small p(x_i)$. 则==**平均码字长度**==可定义为：$$\small \overline{R} = \sum_{i=1}^L l_i p(x_i),$$ 其中，$\small l_i$ 为表示符号 $\small x_i$ 的码字长度。
- **==信源编码定理==**：假设 DMS 信源 $\small X$ 如上，其熵为 $\small H(X)$，则可创建一个**满足前缀条件**的编码，且该编码的**平均码字长度**满足如下条件：$$\small H(X)\leqslant \overline{R} < H(X)+1.$$

> **注：** 信源编码定理告诉我们，使用**任意**的前缀码来表示信源输出符号，其平均码字长度的**最小值**为信源的熵，即如果我们找到一种编码方式，其平均码字长度等于信源熵，那么这种编码方式必定为**最优方式**！

- **前缀码的编码效率**： $$\small \eta = \frac{H(X)}{\overline{R}},\ (\color{blue}{\eta\leqslant 1}).$$

> **注：** 编码效率衡量了对于信源数据的==**压缩**==。事实上对信源进行编码最初就是为了压缩数据（图像，声音，文字等）。


### **如何构造前缀码**？
- **0，1 二叉树**
![TreeFig](media/16018898316337/TreeFig.png)

- **利用0，1二叉树构造前缀码**：需要产生 $\small L$ 个符号的码字集合，则二叉树需要 $\small L$ 层。

## Huffman 编码
这一节考虑如何对**非等可能输出的DMS信源**进行==高效==编码。1952年，Huffman提出了一套基于信源输出符号概率 $\small p(x_i),\ i=1,2,\cdots, L$ 的可变长度编码算法——==**Huffman 编码**==。

- **Huffman算法.** 

```
  （i）排序：将信源符号按照概率从高到低排序；
 （ii）取最底端两个符号做成两个分支，概率大的一支标记‘0’，小的一支标记‘1’，再将两分支合并成一个分支，该分支的概率为前两个分支概率之和；
（iii）将新的分支节点作为一个新节点，重复步骤（ii），直到没有信源符号剩余为止；
 （iv）从最后一个节点向后回溯，便可得到对应符号的Huffman编码，此编码为前缀码。
```

- **示例**： 考虑一个7种输出的 DMS 信源，输出符号记为 $\small x_i,\ i=1,2,\cdots, 7,$ 且对应的概率分别为：$\small p(x_1) = 0.37$, $\small p(x_2) = 0.33$, $\small p(x_3) = 0.16$, $\small p(x_4) = 0.07$, $\small p(x_5) = 0.04$, $\small p(x_6) = 0.02$, $\small p(x_7) = 0.01$. Huffman 算法如下图：

 ![Huffman](media/16018898316337/Huffman.png)

如上图，Huffman算法得到了一个**树结构**， 称为**==Huffman 树==**，根据Huffman树，我们可以得到七种符号的Huffman编码：

<style>
table th:first-of-type {
    width: 4cm;
}
table th:nth-of-type(2) {
    width: 150pt;
}
table th:nth-of-type(3) {
    width: 8em;
}
</style>

| 符号 | 概率 | 码字 |
| --- | --- | --- |
| $\small x_1$ | $\small 0.37$ | 0 |
| $\small x_2$ | $\small 0.33$ | 10 |
| $\small x_3$ | $\small 0.16$ | 110 |
| $\small x_4$ | $\small 0.07$ | 1110 |
| $\small x_5$ | $\small 0.04$ | 11110 |
| $\small x_6$ | $\small 0.02$ | 111110 |
| $\small x_7$ | $\small 0.01$ | 111111 |

该信源的**==熵==**为：$$\small H(X) = -\sum_{i=1}^7 p(x_i)\log_2 p(x_i) \approx 2.12\  \mathrm{bits},$$
==**平均码字长度**==为： $$\small \overline{R} = \sum_{i=1}^7 l_i p(x_i) = 2.17\ \mathrm{bits},$$
==**编码效率**==为：$$\small \eta = \frac{H(X)}{\overline{R}} \approx 0.97.$$

- **Huffman 编码何时达到最优？**
$$\small
\begin{split}
 \overline{R} = \sum_{i=1}^L &l_i p(x_i){\color{red} = }-\sum_{i=1}^L p(x_i)\log_2 p(x_i) = H(X)\\
&\Longrightarrow \color{red}{p(x_i) = 2^{-l_i}},\ i = 1,2,\cdots, L.
\end{split}
$$即，对于Huffman编码，码字长度 $\small l_i$ 根据算法确定，因此只有当每个符号输出**==概率为2的负整数幂==**时才能达到最优编码效率（$\small \eta = 1$）. 

> **注：** 在实际情况中，我们无法修改或控制信源输出符号的概率，因此Huffman编码就很难达到最优编码效率，因此需要更为高效的编码——**算术编码**。


## **算术编码**
上一节我们介绍了Huffman编码，我们发现这套编码方式具有即时性，唯一可译码性，但是对于信源而言依然不是最优的，只有当输出概率为特定的值时能够达到最优，其==根本原因为Huffman编码是基于0，1比特位进行编码。==因此想得到更高效的编码方式就需要打破比特位编码思想，即能否利用实数进行编码？答案是肯定的，这就是==**算术编码**（Arithmetic Coding）==。

- **算术编码过程示例：**  假设信源字母表为$\small A, B, C$，输出概率为$\small p(A) = 0.75$, $\small p(B) = 0.25$, $\small p(C) = 0.25$. 希望传递$\small BACA$，其算术编码过程如下：

![arithmeticCoding](media/16018898316337/arithmeticCoding.png)

- **关键之处：** 
    - 对区间$\small [0, 1)$按照概率比例进行分段，每一段就对应一个字母；
    - 选定某个字母后，再对该字母所在线段按照概率比例分段，以此类推；
    - 选完最后一个字母时我们得到的**依然是一个小区间**，如上例中的$\small [0.59375, 0.609375)$，则该区间中的**任意一个数就代表了整个字符串**$\small BACA$. 比如我们可以选择$\small 0.59375$作为字符串的算术编码。

- **算术编码的==译码==过程：**
    - **接收方**必须要知道信源字母输出概率；
    - 按照上图顺序，看接收到的数字属于哪一个区间，则该区间所对应的字母就是接收到的第一个字符；
    - 再将该区间继续按照概率比例细分，与接收到的数字比对，选择数字所在的区间所对应的字母便是接收到的第二个字符，以此类推...
    - ==**BUG！**== **何时终止**译码？
        - **解决方案：** 当所得到的细分区间长度小于一个事先规定的长度（比如上例中可规定最小长度为0.02）则认为译码终止，即信源传递的字符串结束。

> **注：** 显然，对于任意长度的字符串，算术编码仅用一个数就可以对其进行编码，这对于信源方是最优的选择，并且算术编码的译码过程是一个瞬时无延迟的过程，**但是**，在译码的==唯一性方面就会有一定的误差==，由于终止条件是接收方给定的，因此容易造成翻译的字符串比原本要传递的字符串==多（或少）==。
> 因此，总的来说任何一种信源编码方式都无法做到十全十美，编码效率达到最优那必定会对译码过程带来一定的影响，因此在实际选择编码方式时要综合考虑，取**==中庸之道！==**
> 还有很多应用广泛的编码方式，如**[L-Z编码和游码编码](https://github.com/RuiZhangJerry/ImformationTheory)**。



